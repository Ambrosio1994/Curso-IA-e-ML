{"cells":[{"cell_type":"markdown","metadata":{"id":"tHvwqIC5FkV9"},"source":["<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-3-public/blob/main/Course%202%20-%20Custom%20Training%20loops%2C%20Gradients%20and%20Distributed%20Training/Week%204%20-%20Distribution%20Strategy/C2_W4_Lab_2_multi-GPU-mirrored-strategy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"QEdC1X01FkWE"},"source":["# Multi-GPU Mirrored Strategy\n","\n","In this ungraded lab, you'll go through how to set up a Multi-GPU Mirrored Strategy. The lab environment only has a CPU but we placed the code here in case you want to try this out for yourself in a multiGPU device.\n","\n","**Notes:**\n","- If you are running this on Coursera, you'll see it gives a warning about no presence of GPU devices.\n","- If you are running this in Colab, make sure you have selected your `runtime` to be `GPU`.\n","- In both these cases, you'll see there's only 1 device that is available.  \n","- One device is sufficient for helping you understand these distribution strategies."]},{"cell_type":"markdown","metadata":{"id":"3zisKFyCFkWF"},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"_u3j2HPEFkWG","executionInfo":{"status":"ok","timestamp":1728490968534,"user_tz":180,"elapsed":6657,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import os"]},{"cell_type":"markdown","metadata":{"id":"_3EVG-MMFkWJ"},"source":["## Setup Distribution Strategy"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sGZ3KQJyFkWJ","executionInfo":{"status":"ok","timestamp":1728490968536,"user_tz":180,"elapsed":22,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}},"outputId":"93f670a2-caf2-414c-eac1-df8d3f0ee9bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of devices: 1\n"]}],"source":["# Note that it generally has a minimum of 8 cores, but if your GPU has\n","# less, you need to set this. In this case one of my GPUs has 4 cores\n","os.environ[\"TF_MIN_GPU_MULTIPROCESSOR_COUNT\"] = \"4\"\n","\n","# If the list of devices is not specified in the\n","# `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.\n","# If you have *different* GPUs in your system, you probably have to set up cross_device_ops like this\n","strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n","print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))"]},{"cell_type":"markdown","metadata":{"id":"oLLpanaCFkWK"},"source":["## Prepare the Data"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VSzwMsixFkWL","executionInfo":{"status":"ok","timestamp":1728490977441,"user_tz":180,"elapsed":8918,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}},"outputId":"25fc0f79-7bb8-4e89-b3b2-b33be0d78c12"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"]}],"source":["# Get the data\n","fashion_mnist = tf.keras.datasets.fashion_mnist\n","(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n","\n","# Adding a dimension to the array -> new shape == (28, 28, 1)\n","# We are doing this because the first layer in our model is a convolutional\n","# layer and it requires a 4D input (batch_size, height, width, channels).\n","# batch_size dimension will be added later on.\n","train_images = train_images[..., None]\n","test_images = test_images[..., None]\n","\n","# Normalize the images to [0, 1] range.\n","train_images = train_images / np.float32(255)\n","test_images = test_images / np.float32(255)\n","\n","# Batch the input data\n","BUFFER_SIZE = len(train_images)\n","BATCH_SIZE_PER_REPLICA = 64\n","GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n","\n","# Create Datasets from the batches\n","train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\n","test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)\n","\n","# Create Distributed Datasets from the datasets\n","train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n","test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)"]},{"cell_type":"markdown","metadata":{"id":"gsnTwLdwFkWL"},"source":["## Define the Model"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"N8gEVHQ6FkWM","executionInfo":{"status":"ok","timestamp":1728490977443,"user_tz":180,"elapsed":13,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}}},"outputs":[],"source":["# Create the model architecture\n","def create_model():\n","  model = tf.keras.Sequential([\n","      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n","      tf.keras.layers.MaxPooling2D(),\n","      tf.keras.layers.Conv2D(64, 3, activation='relu'),\n","      tf.keras.layers.MaxPooling2D(),\n","      tf.keras.layers.Flatten(),\n","      tf.keras.layers.Dense(64, activation='relu'),\n","      tf.keras.layers.Dense(10)\n","    ])\n","  return model"]},{"cell_type":"markdown","metadata":{"id":"acsJpKg0FkWM"},"source":["## Configure custom training\n","\n","Instead of `model.compile()`, we're going to do custom training, so let's do that within a strategy scope."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"1oMExDxlFkWN","executionInfo":{"status":"ok","timestamp":1728490977444,"user_tz":180,"elapsed":13,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}}},"outputs":[],"source":["with strategy.scope():\n","    # We will use sparse categorical crossentropy as always. But, instead of having the loss function\n","    # manage the map reduce across GPUs for us, we'll do it ourselves with a simple algorithm.\n","    # Remember -- the map reduce is how the losses get aggregated\n","    # Set reduction to `none` so we can do the reduction afterwards and divide byglobal batch size.\n","    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n","\n","    def compute_loss(labels, predictions):\n","        # Compute Loss uses the loss object to compute the loss\n","        # Notice that per_example_loss will have an entry per GPU\n","        # so in this case there'll be 2 -- i.e. the loss for each replica\n","        per_example_loss = loss_object(labels, predictions)\n","        # You can print it to see it -- you'll get output like this:\n","        # Tensor(\"sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(48,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0)\n","        # Tensor(\"replica_1/sparse_categorical_crossentropy/weighted_loss/Mul:0\", shape=(48,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)\n","        # Note in particular that replica_0 isn't named in the weighted_loss -- the first is unnamed, the second is replica_1 etc\n","        print(per_example_loss)\n","        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n","\n","    # We'll just reduce by getting the average of the losses\n","    test_loss = tf.keras.metrics.Mean(name='test_loss')\n","\n","    # Accuracy on train and test will be SparseCategoricalAccuracy\n","    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n","\n","    # Optimizer will be Adam\n","    optimizer = tf.keras.optimizers.Adam()\n","\n","    # Create the model within the scope\n","    model = create_model()"]},{"cell_type":"markdown","metadata":{"id":"TsNJ87N3FkWO"},"source":["## Train and Test Steps Functions\n","\n","Let's define a few utilities to facilitate the training."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"k0VOdqKP2NEz","executionInfo":{"status":"ok","timestamp":1728490977444,"user_tz":180,"elapsed":12,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}}},"outputs":[],"source":["# `run` replicates the provided computation and runs it\n","# with the distributed input.\n","@tf.function\n","def distributed_train_step(dataset_inputs):\n","  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n","  #tf.print(per_replica_losses.values)\n","  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n","\n","def train_step(inputs):\n","  images, labels = inputs\n","  with tf.GradientTape() as tape:\n","    predictions = model(images, training=True)\n","    loss = compute_loss(labels, predictions)\n","\n","  gradients = tape.gradient(loss, model.trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","  train_accuracy.update_state(labels, predictions)\n","  return loss\n","\n","#######################\n","# Test Steps Functions\n","#######################\n","@tf.function\n","def distributed_test_step(dataset_inputs):\n","  return strategy.run(test_step, args=(dataset_inputs,))\n","\n","def test_step(inputs):\n","  images, labels = inputs\n","\n","  predictions = model(images, training=False)\n","  t_loss = loss_object(labels, predictions)\n","\n","  test_loss.update_state(t_loss)\n","  test_accuracy.update_state(labels, predictions)"]},{"cell_type":"markdown","metadata":{"id":"6LYcQsSRFkWP"},"source":["## Training Loop\n","\n","We can now start training the model."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3L0bEvcIFkWP","executionInfo":{"status":"ok","timestamp":1728491637711,"user_tz":180,"elapsed":583797,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}},"outputId":"921325b2-1a4d-4b30-b2cb-9c974ddd2836"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 0.33401742577552795, Accuracy: 84.71333312988281, Test Loss: 0.35428252816200256, Test Accuracy: 87.3550033569336\n","Epoch 2, Loss: 0.2906759977340698, Accuracy: 89.4566650390625, Test Loss: 0.29511773586273193, Test Accuracy: 89.55000305175781\n","Epoch 3, Loss: 0.2603362798690796, Accuracy: 90.45833587646484, Test Loss: 0.2906980812549591, Test Accuracy: 89.41000366210938\n","Epoch 4, Loss: 0.23618775606155396, Accuracy: 91.40166473388672, Test Loss: 0.2696342170238495, Test Accuracy: 90.30999755859375\n","Epoch 5, Loss: 0.2167814075946808, Accuracy: 92.0616683959961, Test Loss: 0.2728011906147003, Test Accuracy: 90.10000610351562\n","Epoch 6, Loss: 0.20113538205623627, Accuracy: 92.62333679199219, Test Loss: 0.33195173740386963, Test Accuracy: 87.70999908447266\n","Epoch 7, Loss: 0.18613848090171814, Accuracy: 93.19166564941406, Test Loss: 0.25894904136657715, Test Accuracy: 90.69999694824219\n","Epoch 8, Loss: 0.17152048647403717, Accuracy: 93.65166473388672, Test Loss: 0.2496033012866974, Test Accuracy: 91.22000122070312\n","Epoch 9, Loss: 0.15833161771297455, Accuracy: 94.08000183105469, Test Loss: 0.2574269473552704, Test Accuracy: 91.05999755859375\n","Epoch 10, Loss: 0.1450146734714508, Accuracy: 94.64666748046875, Test Loss: 0.25795605778694153, Test Accuracy: 91.29999542236328\n"]}],"source":["EPOCHS = 10\n","for epoch in range(EPOCHS):\n","  # Do Training\n","  total_loss = 0.0\n","  num_batches = 0\n","  for batch in train_dist_dataset:\n","    total_loss += distributed_train_step(batch)\n","    num_batches += 1\n","  train_loss = total_loss / num_batches\n","\n","  # Do Testing\n","  for batch in test_dist_dataset:\n","    distributed_test_step(batch)\n","\n","  template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \" \"Test Accuracy: {}\")\n","\n","  print (template.format(epoch+1, train_loss, train_accuracy.result()*100, test_loss.result(), test_accuracy.result()*100))\n","\n","  test_loss.reset_state()\n","  train_accuracy.reset_state()\n","  test_accuracy.reset_state()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}