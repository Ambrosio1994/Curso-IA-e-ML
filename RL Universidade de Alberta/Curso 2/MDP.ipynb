{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPeBwuTDpPJDzpaHZNjRaHo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<h1>Pol√≠tica (œÄ):</h1>\n","\n","Uma pol√≠tica √© uma estrat√©gia que define o comportamento do agente em um determinado ambiente.\n","Mais especificamente, uma pol√≠tica determina qual a√ß√£o o agente deve tomar em cada estado para maximizar a recompensa cumulativa ao longo do tempo.\n","\n","\n","\n","\n","<h3>Pol√≠tica Determin√≠stica (ùúã):</h3>\n","\n","Define uma a√ß√£o espec√≠fica para cada estado, √© a a√ß√£o que o agente deve tomar quando est√° no estado S.\n","\n","\n","\n","\n","\n","<h3>Pol√≠tica Estoc√°stica (ùúã(a‚à£s)):</h3>\n","\n","Define uma distribui√ß√£o de probabilidade sobre as a√ß√µes para cada estado, √© a probabilidade de o agente tomar a a√ß√£o A no estado S."],"metadata":{"id":"cnSkbXY_RR86"}},{"cell_type":"markdown","source":["<h1>As fun√ß√µes de valor:</h1>\n","\n","Uma fun√ß√£o de valor em aprendizagem por refor√ßo (Reinforcement Learning, RL) tem o objetivo de estimar a \"qualidade\" de estados ou a√ß√µes em termos de recompensas futuras esperadas.\n","\n","Permitem que o agente consulte a qualidade do estado atual (S)\n","em vez de esperar para observar o resultado a longo prazo.\n","Elas calculam todos os retornos poss√≠veis calculando a m√©dia das recompenas esperadas.\n"],"metadata":{"id":"x5f7pwx6F2lw"}},{"cell_type":"markdown","source":["<h1>A fun√ß√£o de valor do estado:</h1>\n","\n","Calcula a recompensa total esperada que um agente pode obter a partir desse estado,\n","seguindo uma pol√≠tica espec√≠fica.\n","\n","√â o pr√™mio futuro que o agente pode esperar receber a partir de um determinado estado.\n","\n","Representa o valor esperado de estar em um detrminado estado."],"metadata":{"id":"oNEGyRREC0Lh"}},{"cell_type":"markdown","source":["<h1>A fun√ß√£o de valor de a√ß√£o:</h1>\n","\n","Calcula a recompensa total esperada que um agente pode obter ap√≥s tomar a a√ß√£o\n","A a partir do estado S e seguir uma pol√≠tica espec√≠fica.\n","\n","Representa o valor esperado de executar uma a√ß√£o A em um estado e seguir a politica\n","\n","Descreve o que acontece quando o agente seleciona pela 1¬™ vez uma a√ß√£o.\n","\n","Nos permite avaliar a probabilidade de recomepensa em cada estado poss√≠vel."],"metadata":{"id":"Fp3NFIHAE9K2"}},{"cell_type":"markdown","source":["<h1>Fun√ß√£o de Valor do Estado V(s):</h1>\n","\n","Estima o valor de estar em um estado espec√≠fico s, independentemente da a√ß√£o tomada.\n","\n","<h1>Fun√ß√£o de Valor de A√ß√£o Q(s,a):</h1>\n","\n","Estima o valor de tomar uma a√ß√£o espec√≠fica a em um estado s."],"metadata":{"id":"FQBk2wT5kUNU"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"5iK7YRvr7l8u","executionInfo":{"status":"ok","timestamp":1717115983933,"user_tz":180,"elapsed":22,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Definindo a fun√ß√£o de recompensa\n","def reward(state):\n","    return 1 if state == 0 else 0"],"metadata":{"id":"mjcK4WgS7nk1","executionInfo":{"status":"ok","timestamp":1717115983934,"user_tz":180,"elapsed":20,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Definindo a fun√ß√£o de transi√ß√£o\n","def next_state(state, action):\n","    row, col = divmod(state, 4)\n","    if action == 0 and row > 0:  # Cima\n","        row -= 1\n","    elif action == 1 and row < 3:  # Baixo\n","        row += 1\n","    elif action == 2 and col > 0:  # Esquerda\n","        col -= 1\n","    elif action == 3 and col < 3:  # Direita\n","        col += 1\n","    return row * 4 + col"],"metadata":{"id":"tadVk4aW7qJO","executionInfo":{"status":"ok","timestamp":1717115983935,"user_tz":180,"elapsed":20,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Inicializando a pol√≠tica aleat√≥ria (probabilidade igual para cada a√ß√£o)\n","policy = np.ones((16, 4)) / 4 # Pol√≠tica inicial uniforme\n","\n","# Inicializando os valores\n","q_values = np.zeros(16)\n","size = q_values.shape[0]"],"metadata":{"id":"U59apEGN8MX9","executionInfo":{"status":"ok","timestamp":1717116065827,"user_tz":180,"elapsed":301,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def policy_evaluation(policy, values, discount_factor=0.9, theta=0.001):\n","    while True:  # Loop principal de avalia√ß√£o da pol√≠tica\n","        delta = 0  # Inicializa delta para monitorar a mudan√ßa m√°xima nos valores de estado\n","\n","        for s in range(size):  # Itera por todos os estados\n","            v = values[s]  # Armazena o valor antigo do estado s\n","            # Calcula o novo valor do estado s sob a pol√≠tica atual\n","            new_value = sum(policy[s][a] * (reward(next_state(s, a)) + discount_factor * values[next_state(s, a)]) for a in range(4))\n","            values[s] = new_value  # Atualiza o valor do estado s\n","            delta = max(delta, abs(v - new_value))  # Atualiza delta com a maior mudan√ßa observada\n","\n","        # Verifica se a mudan√ßa m√°xima nos valores de estado √© menor que o limiar theta\n","        if delta < theta:\n","            break  # Se sim, sai do loop\n","\n","    return values  # Retorna os valores dos estados ap√≥s a avalia√ß√£o"],"metadata":{"id":"gWc5jpc-8Nia","executionInfo":{"status":"ok","timestamp":1717115983936,"user_tz":180,"elapsed":18,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def policy_improvement(policy, values, discount_factor=0.9):\n","    policy_stable = True  # Inicializa a vari√°vel que verifica se a pol√≠tica √© est√°vel\n","    for s in range(size):  # Itera por todos os estados\n","        old_action = np.argmax(policy[s])  # Obt√©m a a√ß√£o antiga (a√ß√£o com maior probabilidade)\n","        action_values = np.zeros(4)  # Inicializa o array para armazenar os valores das a√ß√µes\n","        for a in range(4):  # Itera por todas as a√ß√µes poss√≠veis\n","            # Calcula o valor esperado da a√ß√£o a a partir do estado s\n","            action_values[a] = reward(next_state(s, a)) + discount_factor * values[next_state(s, a)]\n","        best_action = np.argmax(action_values)  # Seleciona a melhor a√ß√£o com base nos valores calculados\n","        if old_action != best_action:  # Verifica se a a√ß√£o mudou\n","            policy_stable = False  # Se mudou, a pol√≠tica n√£o √© est√°vel\n","        policy[s] = np.eye(4)[best_action]  # Atualiza a pol√≠tica para o estado s com a melhor a√ß√£o\n","    return policy, policy_stable  # Retorna a nova pol√≠tica e se ela √© est√°vel"],"metadata":{"id":"K8jzcBI78g-P","executionInfo":{"status":"ok","timestamp":1717115983936,"user_tz":180,"elapsed":18,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def policy_iteration(policy, values, discount_factor=0.9, theta=0.001):\n","    while True:  # Loop principal da itera√ß√£o de pol√≠ticas\n","        # Avalia√ß√£o da pol√≠tica: calcula os valores dos estados sob a pol√≠tica atual\n","        values = policy_evaluation(policy, values, discount_factor, theta)\n","\n","        # Melhoria da pol√≠tica: ajusta a pol√≠tica com base nos valores calculados\n","        policy, policy_stable = policy_improvement(policy, values, discount_factor)\n","\n","        # Verifica se a pol√≠tica se estabilizou (n√£o houve mudan√ßas)\n","        if policy_stable:\n","            break  # Se a pol√≠tica √© est√°vel, sai do loop\n","    return policy, values  # Retorna a pol√≠tica e os valores √≥timos"],"metadata":{"id":"9YjSUiKt8hfv","executionInfo":{"status":"ok","timestamp":1717115983936,"user_tz":180,"elapsed":18,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Executar a itera√ß√£o de pol√≠ticas\n","optimal_policy, optimal_values = policy_iteration(policy, q_values)\n","\n","print(\"cima: 0, baixo: 1, esquera: 2, direita: 3\")\n","# Mostrar a pol√≠tica √≥tima e os valores √≥timos\n","print(\"Pol√≠tica √ìtima (melhores a√ß√µes para cada estado):\")\n","optimal_actions = np.argmax(optimal_policy, axis=1).reshape(4, 4)\n","print(optimal_actions)\n","print(\"\\nValores √ìtimos dos Estados:\")\n","optimal_values = optimal_values.reshape(4, 4)\n","print(optimal_values)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0w2fBzD68jYZ","executionInfo":{"status":"ok","timestamp":1717115983936,"user_tz":180,"elapsed":17,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}},"outputId":"5790da36-30c7-4f7a-c368-0d8c6566acee"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["cima: 0, baixo: 1, esquera: 2, direita: 3\n","Pol√≠tica √ìtima (melhores a√ß√µes para cada estado):\n","[[0 2 2 2]\n"," [0 0 0 0]\n"," [0 0 0 0]\n"," [0 0 0 0]]\n","\n","Valores √ìtimos dos Estados:\n","[[9.99233945 9.9931055  8.99379495 8.09441546]\n"," [9.9931055  8.99379495 8.09441546 7.28497391]\n"," [8.99379495 8.09441546 7.28497391 6.55647652]\n"," [8.09441546 7.28497391 6.55647652 5.90082887]]\n"]}]}]}