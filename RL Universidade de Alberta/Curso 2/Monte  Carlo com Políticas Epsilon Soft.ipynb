{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOFrHENP5ZPQyhtrlTmPW35"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<h1>Políticas Epsilon Soft (ou políticas ε-soft):</h1><p> são um tipo de política no aprendizado por reforço que balanceia a exploração e a exploração das ações. Essas políticas garantem que o agente explore diferentes ações com uma probabilidade mínima, evitando que fique preso a uma política subótima."],"metadata":{"id":"ASDv1WRA-ZHH"}},{"cell_type":"markdown","source":["<h1>Funcionamento de uma Política Epsilon Soft</h1>\n","<h2>Em cada estado, o agente escolhe uma ação:</h2><p>\n","Com probabilidade ε, escolhe uma ação aleatoriamente (exploração).<p>\n","Com probabilidade (1 - ε), escolhe a melhor ação conhecida (exploração).\n"],"metadata":{"id":"SFhvd9v4-5qj"}},{"cell_type":"markdown","source":["<h1>Vantagens</h1>\n","<h2>Evita Convergência Prematura:</h2><p> Garante que todas as ações sejam exploradas em algum grau, evitando que o agente fique preso em um máximo local.<p>\n","<h2>Melhora a Aprendizagem:</h2><p> Ao explorar diferentes ações, o agente pode descobrir políticas melhores que não seriam encontradas se sempre seguisse a política atual."],"metadata":{"id":"LxvtIB18_L4o"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2TEbl9tR2dfe"},"outputs":[],"source":["import numpy as np\n","\n","# Parâmetros\n","num_states = 5\n","num_actions = 2\n","epsilon = 0.1  # Parâmetro epsilon para a política epsilon-soft\n","alpha = 0.1  # Taxa de aprendizagem\n","gamma = 0.9  # Fator de desconto\n","num_episodes = 1000"]},{"cell_type":"code","source":["# Inicialização das funções de valor Q(s, a)\n","Q = np.zeros((num_states, num_actions))\n","Q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OI1vzYJe-gJi","executionInfo":{"status":"ok","timestamp":1716930417384,"user_tz":180,"elapsed":284,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}},"outputId":"29d303ce-6a5c-4482-9c80-eca691ab6cf1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0.],\n","       [0., 0.],\n","       [0., 0.],\n","       [0., 0.],\n","       [0., 0.]])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Política epsilon-greedy\n","def epsilon_greedy_policy(state):\n","    if np.random.rand() < epsilon:\n","        return np.random.randint(num_actions)  # Exploração: ação aleatória\n","    else:\n","        return np.argmax(Q[state])  # Exploração: melhor ação conhecida"],"metadata":{"id":"Gtnx2p4PA3c3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Função de transição do ambiente (simplificada)\n","def transition(state, action):\n","    next_state = (state + action) % num_states  # Exemplo de transição\n","    reward = np.random.randn()  # Recompensa aleatória\n","    return next_state, reward"],"metadata":{"id":"1v4t8-OaA_Qg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Simulação de episódios usando política epsilon-greedy\n","def simulate_episode(start_state=0):\n","    episode = []\n","    state = start_state\n","    while state != 4:  # Suponha que o estado 4 é o estado terminal\n","        action = epsilon_greedy_policy(state)\n","        next_state, reward = transition(state, action)\n","        episode.append((state, action, reward))\n","        state = next_state\n","    return episode"],"metadata":{"id":"OKUZnfPmBCw4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for _ in range(num_episodes):\n","    start_state = np.random.randint(0, num_states)\n","    episode = simulate_episode(start_state)\n","    for state, action, reward in episode:\n","        next_state = (state + action) % num_states\n","        best_next_action = np.argmax(Q[next_state])\n","        td_target = reward + gamma * Q[next_state, best_next_action]\n","        td_error = td_target - Q[state, action]\n","        Q[state, action] += alpha * td_error\n","\n","print(\"Estimativa da função Q(s, a):\")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JMCisXxZBKIr","executionInfo":{"status":"ok","timestamp":1716930489545,"user_tz":180,"elapsed":302,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}},"outputId":"6abe5723-035c-4c8c-f043-15238cbc29dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Estimativa da função Q(s, a):\n","[[-0.58180673 -0.22483632]\n"," [ 0.49415017 -0.20042607]\n"," [-0.47237721  0.34583118]\n"," [-0.70540032 -0.0633433 ]\n"," [ 0.          0.        ]]\n"]}]}]}