{"cells":[{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"4f046aa616adb1b5cff666e8a39e2da6","grade":false,"grade_id":"cell-41f41eebcd73cca6","locked":true,"schema_version":3,"solution":false,"task":false},"id":"MN_7wIv4PcZU"},"source":["# Tarefa: Avaliação de Políticas em Ambiente de Cliff Walking\n","\n","Bem-vindo à Tarefa de Programação do Módulo 2 do Curso 2! Nesta tarefa, você implementará um dos agentes fundamentais de aprendizagem por reforço livre de modelo baseado em amostra e bootstrapping para previsão. Este é aquele que usa aprendizagem por diferença temporal em uma etapa, também conhecido como TD(0). A tarefa é projetar um agente para avaliação de políticas no ambiente Cliff Walking. Lembre-se de que a avaliação de políticas é o problema de previsão em que o objetivo é estimar com precisão os valores dos estados dada alguma política.\n","\n","### Objetivos de aprendizado\n","- Implementar partes do ambiente Cliff Walking, para obter experiência na especificação de MDPs [Seção 1].\n","- Implementar um agente que utilize bootstrapping e, particularmente, TD(0) [Seção 2].\n","- Aplicar TD(0) para estimar funções de valor para diferentes políticas, ou seja, realizar experiências de avaliação de políticas [Secção 3]."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"b116f0285911fc08ee56b1e0f30f0001","grade":false,"grade_id":"cell-be83ec3ee232897b","locked":true,"schema_version":3,"solution":false,"task":false},"id":"RIAdcHNqPcZb"},"source":["## The Cliff Walking Environment\n","\n","O ambiente Cliff Walking é um mundo em grade com um espaço de estado discreto e um espaço de ação discreto. O agente começa na célula S da grade. O agente pode mover-se (deterministicamente) para as quatro células vizinhas executando ações para cima, para baixo, para a esquerda ou para a direita. Tentar sair dos limites resulta em permanecer no mesmo local. Assim, por exemplo, tentar mover-se para a esquerda quando estiver em uma célula da coluna mais à esquerda resulta em nenhum movimento e o agente permanece no mesmo local. O agente recebe -1 de recompensa por etapa na maioria dos estados e -100 de recompensa ao cair do penhasco. Esta é uma tarefa episódica; o término ocorre quando o agente atinge a célula G da grade objetivo. A queda do penhasco resulta na redefinição ao estado inicial, sem término.\n","\n","O diagrama abaixo mostra a descrição acima e também ilustra duas das políticas que iremos avaliar.\n","\n","<img src=\"cliffwalk.png\" style=\"height:400px\">"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"3d66ea9338a76a07ea21c1d22c0202db","grade":false,"grade_id":"cell-a57c99064bb8efe6","locked":true,"schema_version":3,"solution":false,"task":false},"id":"8M7EQ-vwPcZc"},"source":["## Packages.\n","\n","We import the following libraries that are required for this assignment. We shall be using the following libraries:\n","1. jdc: Jupyter magic that allows defining classes over multiple jupyter notebook cells.\n","2. numpy: the fundamental package for scientific computing with Python.\n","3. matplotlib: the library for plotting graphs in Python.\n","4. RL-Glue: the library for reinforcement learning experiments.\n","5. BaseEnvironment, BaseAgent: the base classes from which we will inherit when creating the environment and agent classes in order for them to support the RL-Glue framework.\n","6. Manager: the file allowing for visualization and testing.\n","7. itertools.product: the function that can be used easily to compute permutations.\n","8. tqdm.tqdm: Provides progress bars for visualizing the status of loops.\n","\n","**Please do not import other libraries** this will break the autograder.\n","\n","**NOTE: For this notebook, there is no need to make any calls to methods of random number generators. Spurious or missing calls to random number generators may affect your results.**"]},{"cell_type":"code","source":["!pip install jdc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zIkMRCbGRf56","executionInfo":{"status":"ok","timestamp":1718317410209,"user_tz":180,"elapsed":6505,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}},"outputId":"40523752-b7ea-45c4-982c-aa4bb37b6e87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: jdc in /usr/local/lib/python3.10/dist-packages (0.0.9)\n"]}]},{"cell_type":"code","source":["!pip install manage.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ua1ShEOWSgQB","executionInfo":{"status":"ok","timestamp":1718317422969,"user_tz":180,"elapsed":12783,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}},"outputId":"f6f3acfa-bcb0-4e6d-bfb0-3f37083f1b95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: manage.py in /usr/local/lib/python3.10/dist-packages (0.2.10)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"961301eba64117ca89220f6130255741","grade":false,"grade_id":"cell-880b4e1ed6e705b9","locked":true,"schema_version":3,"solution":false,"task":false},"id":"T_1YvTawPcZd"},"outputs":[],"source":["# Do not modify this cell!\n","\n","import jdc\n","import numpy as np\n","from rl_glue import RLGlue\n","from agent import BaseAgent\n","from environment import BaseEnvironment\n","from manager import Manager\n","from itertools import product\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"25ea26e935a037c9cc5305c35f72e365","grade":false,"grade_id":"cell-3c437cccaf36fbde","locked":true,"schema_version":3,"solution":false,"task":false},"id":"N9LpRX1oPcZf"},"source":["## Section 1. Environment\n","\n","In the first part of this assignment, you will get to see how the Cliff Walking environment is implemented. You will also get to implement parts of it to aid your understanding of the environment and more generally how MDPs are specified. In particular, you will implement the logic for:\n"," 1. Converting 2-dimensional coordinates to a single index for the state,\n"," 2. One of the actions (action up), and,\n"," 3. Reward and termination.\n","\n","Given below is an annotated diagram of the environment with more details that may help in completing the tasks of this part of the assignment. Note that we will be creating a more general environment where the height and width positions can be variable but the start, goal and cliff grid cells have the same relative positions (bottom left, bottom right and the cells between the start and goal grid cells respectively).\n","\n","<img src=\"cliffwalk-annotated.png\" style=\"height:400px\">\n","\n","Once you have gone through the code and begun implementing solutions, it may be a good idea to come back here and see if you can convince yourself that the diagram above is an accurate representation of the code given and the code you have written."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"70a921fedaf47f9cd58bcc38dfff2079","grade":false,"grade_id":"cell-61ab5d6bc73499aa","locked":true,"schema_version":3,"solution":false,"task":false},"id":"j7Dl2-MaPcZg"},"outputs":[],"source":["# Create empty CliffWalkEnvironment class.\n","# These methods will be filled in later cells.\n","class CliffWalkEnvironment(BaseEnvironment):\n","    def env_init(self, env_info={}):\n","        raise NotImplementedError\n","\n","    def env_start(self):\n","        raise NotImplementedError\n","\n","    def env_step(self, action):\n","        raise NotImplementedError\n","\n","    def env_cleanup(self):\n","        raise NotImplementedError\n","\n","    # helper method\n","    def state(self, loc):\n","        raise NotImplementedError"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"cdb4802e028fdadfc490acae83c28e4f","grade":false,"grade_id":"cell-787bae36a5a76fc1","locked":true,"schema_version":3,"solution":false,"task":false},"id":"cjwAYZflPcZh"},"source":["## env_init()\n","\n","The first function we add to the environment is the initialization function which is called once when an environment object is created. In this function, the grid dimensions and special locations (start and goal locations and the cliff locations) are stored for easy use later."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"9628a884b34ab00f14b560fa3eddc294","grade":false,"grade_id":"cell-e15c46384ae349b7","locked":true,"schema_version":3,"solution":false,"task":false},"id":"-R2S-LnbPcZi"},"outputs":[],"source":["%%add_to CliffWalkEnvironment\n","\n","def env_init(self, env_info={}):\n","        \"\"\"Setup for the environment called when the experiment first starts.\n","        Note:\n","            Initialize a tuple with the reward, first state, boolean\n","            indicating if it's terminal.\n","        \"\"\"\n","\n","        # Note, we can setup the following variables later, in env_start() as it is equivalent.\n","        # Code is left here to adhere to the note above, but these variables are initialized once more\n","        # in env_start() [See the env_start() function below.]\n","\n","        reward = None\n","        state = None # See Aside\n","        termination = None\n","        self.reward_state_term = (reward, state, termination)\n","\n","        # AN ASIDE: Observation is a general term used in the RL-Glue files that can be interachangeably\n","        # used with the term \"state\" for our purposes and for this assignment in particular.\n","        # A difference arises in the use of the terms when we have what is called Partial Observability where\n","        # the environment may return states that may not fully represent all the information needed to\n","        # predict values or make decisions (i.e., the environment is non-Markovian.)\n","\n","        # Set the default height to 4 and width to 12 (as in the diagram given above)\n","        self.grid_h = env_info.get(\"grid_height\", 4)\n","        self.grid_w = env_info.get(\"grid_width\", 12)\n","\n","        # Now, we can define a frame of reference. Let positive x be towards the direction down and\n","        # positive y be towards the direction right (following the row-major NumPy convention.)\n","        # Then, keeping with the usual convention that arrays are 0-indexed, max x is then grid_h - 1\n","        # and max y is then grid_w - 1. So, we have:\n","        # Starting location of agent is the bottom-left corner, (max x, min y).\n","        self.start_loc = (self.grid_h - 1, 0)\n","        # Goal location is the bottom-right corner. (max x, max y).\n","        self.goal_loc = (self.grid_h - 1, self.grid_w - 1)\n","\n","        # O penhasco conterá todas as células entre start_loc e goal_loc.\n","        self.cliff = [(self.grid_h - 1, i) for i in range(1, (self.grid_w - 1))]\n","\n","        # Take a look at the annotated environment diagram given in the above Jupyter Notebook cell to\n","        # verify that your understanding of the above code is correct for the default case, i.e., where\n","        # height = 4 and width = 12."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"900b8b4c3bbad22ecd0d96c4961ed441","grade":false,"grade_id":"cell-788513ad1e5a7993","locked":true,"schema_version":3,"solution":false,"task":false},"id":"DHN9FEeFPcZj"},"source":["## *Implement* state()\n","    \n","The agent location can be described as a two-tuple or coordinate (x, y) describing the agentâ€™s position.\n","However, we can convert the (x, y) tuple into a single index and provide agents with just this integer.\n","One reason for this choice is that the spatial aspect of the problem is secondary and there is no need\n","for the agent to know about the exact dimensions of the environment.\n","From the agentâ€™s viewpoint, it is just perceiving some states, accessing their corresponding values\n","in a table, and updating them. Both the coordinate (x, y) state representation and the converted coordinate representation are thus equivalent in this sense.\n","\n","Given a grid cell location, the state() function should return the state; a single index corresponding to the location in the grid.\n","\n","\n","```\n","Example: Suppose grid_h is 2 and grid_w is 2. Then, we can write the grid cell two-tuple or coordinate\n","states as follows (following the usual 0-index convention):\n","|(0, 0) (0, 1)| |0 1|\n","|(1, 0) (1, 1)| |2 3|\n","Assuming row-major order as NumPy does,  we can flatten the latter to get a vector [0 1 2 3].\n","So, if loc = (0, 0) we return 0. While, for loc = (1, 1) we return 3.\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"d8812eea241d83eb6a6e102019b543e4","grade":false,"grade_id":"cell-41a1c253613aa0b0","locked":false,"schema_version":3,"solution":true,"task":false},"id":"fu2NUC4bPcZk"},"outputs":[],"source":["%%add_to CliffWalkEnvironment\n","\n","\n","# Modify the return statement of this function to return a correct single index as\n","# the state (see the logic for this in the previous cell.)\n","def state(self, loc):\n","    # your code here\n","    row, col = loc\n","    linear_index = row * self.grid_w + col\n","    return linear_index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IyJNZbFTPcZk"},"outputs":[],"source":["# Feel free to make any changes to this cell to debug your code\n","\n","env = CliffWalkEnvironment()\n","env.env_init({ \"grid_height\": 4, \"grid_width\": 12 })\n","\n","coords = [(0, 0), (0, 11), (1, 5), (3, 0), (3, 9), (3, 11)]\n","correct_outputs = [0, 11, 17, 36, 45, 47]\n","\n","got = [env.state(s) for s in coords]\n","assert got == correct_outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"f405849cc734485507aaaf0a6c3de361","grade":true,"grade_id":"cell-af9c8dc2ad3b0918","locked":true,"points":5,"schema_version":3,"solution":false,"task":false},"id":"1IqhZ603PcZl"},"outputs":[],"source":["# The contents of the cell will be tested by the autograder.\n","# If they do not pass here, they will not pass there.\n","\n","np.random.seed(0)\n","\n","env = CliffWalkEnvironment()\n","for n in range(100):\n","    # make a gridworld of random size and shape\n","    height = np.random.randint(2, 100)\n","    width = np.random.randint(2, 100)\n","    env.env_init({ \"grid_height\": height, \"grid_width\": width })\n","\n","    # generate some random coordinates within the grid\n","    idx_h = np.random.randint(height)\n","    idx_w = np.random.randint(width)\n","\n","    # check that the state index is correct\n","    state = env.state((idx_h, idx_w))\n","    correct_state = width * idx_h + idx_w\n","\n","    assert state == correct_state"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"6b02e570dfc9a3f91deb8844fec71cb6","grade":false,"grade_id":"cell-afaec4ef9dad3e8c","locked":true,"schema_version":3,"solution":false,"task":false},"id":"5-HEikGdPcZl"},"source":["## env_start()\n","\n","In env_start(), we initialize the agent location to be the start location and return the state corresponding to it as the first state for the agent to act upon. Additionally, we also set the reward and termination terms to be 0 and False respectively as they are consistent with the notion that there is no reward nor termination before the first action is even taken."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"9d9555e6777c4238253214ee338d1120","grade":false,"grade_id":"cell-d4a47fc700dc1702","locked":true,"schema_version":3,"solution":false,"task":false},"id":"41cKSWJdPcZl"},"outputs":[],"source":["%%add_to CliffWalkEnvironment\n","\n","def env_start(self):\n","    \"\"\"The first method called when the episode starts, called before the\n","    agent starts.\n","\n","    Returns:\n","        The first state from the environment.\n","    \"\"\"\n","    reward = 0\n","\n","    # agent_loc manterá a localização atual do agente\n","    self.agent_loc = self.start_loc\n","\n","    # state é a representação de estado unidimensional da localização do agente.\n","    state = self.state(self.agent_loc)\n","    termination = False\n","    self.reward_state_term = (reward, state, termination)\n","\n","    return self.reward_state_term[1]"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"59d7de251b98b0d5cf643c8f997ea120","grade":false,"grade_id":"cell-97e8a586d4c74c95","locked":true,"schema_version":3,"solution":false,"task":false},"id":"NAfcSvynPcZm"},"source":["## *Implement* env_step()\n","\n","Once an action is taken by the agent, the environment must provide a new state, reward and termination signal.\n","\n","In the Cliff Walking environment, agents move around using a 4-cell neighborhood called the Von Neumann neighborhood (https://en.wikipedia.org/wiki/Von_Neumann_neighborhood). Thus, the agent has 4 available actions at each state. Three of the actions have been implemented for you and your first task is to implement the logic for the fourth action (Action UP).\n","\n","Your second task for this function is to implement the reward logic. Look over the environment description given earlier in this notebook if you need a refresher for how the reward signal is defined."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"93c59f644f0a24823d6fd512465b83b8","grade":false,"grade_id":"cell-47a55053538ed113","locked":false,"schema_version":3,"solution":true,"task":false},"id":"wUdh6V3JPcZm"},"outputs":[],"source":["%%add_to CliffWalkEnvironment\n","\n","def isInBounds(self, x, y, width, height):\n","    return 0 <= x < height and 0 <= y < width\n","\n","def env_step(self, action):\n","    \"\"\"A step taken by the environment.\"\"\"\n","    x, y = self.agent_loc\n","\n","    # UP\n","    if action == 0:\n","        x = x - 1\n","    # LEFT\n","    elif action == 1:\n","        y = y - 1\n","    # DOWN\n","    elif action == 2:\n","        x = x + 1\n","    # RIGHT\n","    elif action == 3:\n","        y = y + 1\n","    else:\n","        raise Exception(f\"{action} não é uma ação reconhecida [0: Cima, 1: Esquerda, 2: Baixo, 3: Direita]!\")\n","\n","    if not self.isInBounds(x, y, self.grid_w, self.grid_h):\n","        x, y = self.agent_loc\n","\n","    self.agent_loc = (x, y)\n","    reward = -1\n","    terminal = False\n","\n","    # Verifica se o agente cai no penhasco\n","    if self.agent_loc in self.cliff:\n","        reward = -100\n","        terminal = False\n","        self.agent_loc = self.start_loc  # Reseta a posição do agente\n","\n","    elif self.agent_loc == self.goal_loc:\n","        terminal = True\n","\n","    self.reward_state_term = (reward, self.state(self.agent_loc), terminal)\n","    return self.reward_state_term"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ahe1ilGiPcZm"},"outputs":[],"source":["# Feel free to make any changes to this cell to debug your code\n","\n","def test_action_up():\n","    env = CliffWalkEnvironment()\n","    env.env_init({\"grid_height\": 4, \"grid_width\": 12})\n","    env.agent_loc = (0, 0)\n","    env.env_step(0)\n","    assert(env.agent_loc == (0, 0))\n","\n","    env.agent_loc = (1, 0)\n","    env.env_step(0)\n","    assert(env.agent_loc == (0, 0))\n","\n","def test_reward():\n","    env = CliffWalkEnvironment()\n","    env.env_init({\"grid_height\": 4, \"grid_width\": 12})\n","    env.agent_loc = (0, 0)\n","    reward_state_term = env.env_step(0)\n","    assert(reward_state_term[0] == -1 and reward_state_term[1] == env.state((0, 0)) and\n","           reward_state_term[2] == False)\n","\n","    env.agent_loc = (3, 1)\n","    reward_state_term = env.env_step(2)\n","    assert(reward_state_term[0] == -100 and reward_state_term[1] == env.state((3, 0)) and\n","           reward_state_term[2] == False)\n","\n","    env.agent_loc = (2, 11)\n","    reward_state_term = env.env_step(2)\n","    assert(reward_state_term[0] == -1 and reward_state_term[1] == env.state((3, 11)) and reward_state_term[2] == True)\n","\n","test_action_up()\n","test_reward()"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"930e2f004e7f5bc157d4ade45c186185","grade":true,"grade_id":"cell-cc53e967d108b33c","locked":true,"points":5,"schema_version":3,"solution":false,"task":false},"id":"_t8i9U5PPcZn"},"outputs":[],"source":["np.random.seed(0)\n","\n","env = CliffWalkEnvironment()\n","for n in range(100):\n","    # crie um mundo de penhasco de tamanho aleatório\n","    height = np.random.randint(2, 100)\n","    width = np.random.randint(2, 100)\n","    env.env_init({\"grid_height\": height, \"grid_width\": width})\n","\n","    #  inicie o agente em um local aleatório\n","    idx_h = 0 if np.random.random() < 0.5 else np.random.randint(height)\n","    idx_w = np.random.randint(width)\n","    env.agent_loc = (idx_h, idx_w)\n","\n","    env.env_step(0)\n","    assert(env.agent_loc == (0 if idx_h == 0 else idx_h - 1, idx_w))"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"085778d845e6e8259013f384c9c71fc7","grade":true,"grade_id":"cell-a883fea60cb72354","locked":true,"points":10,"schema_version":3,"solution":false,"task":false},"id":"iLtkd_5dPcZn"},"outputs":[],"source":["np.random.seed(0)\n","\n","env = CliffWalkEnvironment()\n","for n in range(100):\n","    # crie um mundo de penhasco de tamanho aleatório\n","    height = np.random.randint(4, 10)\n","    width = np.random.randint(4, 10)\n","    env.env_init({\"grid_height\": height, \"grid_width\": width})\n","    env.env_start()\n","\n","    # inicie o agente perto do penhasco\n","    idx_h = height - 2\n","    idx_w = np.random.randint(1, width - 2)\n","    env.agent_loc = (idx_h, idx_w)\n","\n","    r, sp, term = env.env_step(2)\n","    assert(r == -100 and sp == (height - 1) * width and term == False)\n","\n","for n in range(100):\n","    # crie um mundo de penhasco de tamanho aleatório\n","    height = np.random.randint(4, 10)\n","    width = np.random.randint(4, 10)\n","    env.env_init({\"grid_height\": height, \"grid_width\": width})\n","    env.env_start()\n","\n","    # inicia o agente perto do gol\n","    idx_h = height - 2\n","    idx_w = width - 1\n","    env.agent_loc = (idx_h, idx_w)\n","\n","    r, sp, term = env.env_step(2)\n","    assert(r == -1 and sp == (height - 1) * width + (width - 1) and term == True)\n","\n","for n in range(100):\n","    # crie um mundo de penhasco de tamanho aleatório\n","    height = np.random.randint(4, 10)\n","    width = np.random.randint(4, 10)\n","    env.env_init({\"grid_height\": height, \"grid_width\": width})\n","    env.env_start()\n","\n","    # inicie o agente em um local aleatório\n","    idx_h = np.random.randint(0, height - 3)\n","    idx_w = np.random.randint(0, width - 1)\n","    env.agent_loc = (idx_h, idx_w)\n","\n","    r, sp, term = env.env_step(2)\n","    assert(r == -1 and term == False)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"75a723b12d51aa479c9820a827f78105","grade":false,"grade_id":"cell-097d028e4c6b9ff2","locked":true,"schema_version":3,"solution":false,"task":false},"id":"gyPnjNMLPcZo"},"source":["## env_cleanup()\n","\n","There is not much cleanup to do for the Cliff Walking environment. Here, we simply reset the agent location to be the start location in this function."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"2239673e0e1e0ce4adb6f2d4a6a703e3","grade":false,"grade_id":"cell-d5ff562d7edc277d","locked":true,"schema_version":3,"solution":false,"task":false},"id":"7_fBd0nRPcZo"},"outputs":[],"source":["%%add_to CliffWalkEnvironment\n","\n","def env_cleanup(self):\n","    \"\"\"Limpeza feita após o término do ambiente\"\"\"\n","    self.agent_loc = self.start_loc"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"3143490bb273c21183bb0c2397ed4b53","grade":false,"grade_id":"cell-fc714f9c0311d966","locked":true,"schema_version":3,"solution":false,"task":false},"id":"M9R4Q73APcZo"},"source":["## Section 2. Agent\n","\n","In this second part of the assignment, you will be implementing the key updates for Temporal Difference Learning. There are two cases to consider depending on whether an action leads to a terminal state or not."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"2ecf4ec36550b6e56088c40cb4c6cf63","grade":false,"grade_id":"cell-8dcd58cfaa9b1391","locked":true,"schema_version":3,"solution":false,"task":false},"id":"kJ9w_X-cPcZo"},"outputs":[],"source":["# Cria uma classe TDAgent vazia.\n","# Esses métodos serão preenchidos nas células posteriores.\n","\n","class TDAgent(BaseAgent):\n","    def agent_init(self, agent_info={}):\n","        raise NotImplementedError\n","\n","    def agent_start(self, state):\n","        raise NotImplementedError\n","\n","    def agent_step(self, reward, state):\n","        raise NotImplementedError\n","\n","    def agent_end(self, reward):\n","        raise NotImplementedError\n","\n","    def agent_cleanup(self):\n","        raise NotImplementedError\n","\n","    def agent_message(self, message):\n","        raise NotImplementedError"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"6ad6054dcb773c2b268b93cb01194d61","grade":false,"grade_id":"cell-fb86d23d3049ba9a","locked":true,"schema_version":3,"solution":false,"task":false},"id":"sdQNMBSpPcZp"},"source":["## agent_init()\n","\n","Assim como fizemos com o ambiente, primeiro inicializamos o agente uma vez quando um objeto TDAgent é criado. Nesta função, criamos um gerador de números aleatórios, propagado com a semente fornecida no dicionário agent_info para obter resultados reproduzíveis. Também definimos a política, o desconto e o tamanho do passo com base no dicionário agent_info. Finalmente, com a convenção de que a política é sempre especificada como um mapeamento de estados para ações, assim como uma matriz de tamanho (# Estados, # Ações), inicializamos uma matriz de valores de forma (# Estados,) com zeros."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"7b2bd8f7421b260e24fb2ffea9831ee3","grade":false,"grade_id":"cell-7e6fb425c17b2222","locked":true,"schema_version":3,"solution":false,"task":false},"id":"qL2cOcWOPcZp"},"outputs":[],"source":["%%add_to TDAgent\n","\n","def agent_init(self, agent_info={}):\n","    \"\"\"Configuração do agente chamado quando o experimento é iniciado.\"\"\"\n","\n","    # Crie um gerador de números aleatórios com a semente fornecida para propagar o agente para reprodutibilidade.\n","    self.rand_generator = np.random.RandomState(agent_info.get(\"seed\"))\n","\n","    # A política será dada, lembre-se que o objetivo é estimar com precisão sua função de valor correspondente.\n","    self.policy = agent_info.get(\"policy\")\n","\n","   # Fator de desconto (gama) para utilizar nas atualizações.\n","    self.discount = agent_info.get(\"discount\")\n","\n","    # A taxa de aprendizagem ou parâmetro de tamanho do passo (alfa) a ser usado nas atualizações.\n","    self.step_size = agent_info.get(\"step_size\")\n","\n","    # Inicialize um array de zeros que conterá os valores.\n","    # Lembre-se de que a política pode ser representada como uma matriz (# Estados, # Ações),\n","    # supondo que este seja o caso, podemos usar a primeira dimensão da política para\n","    # inicializa o array para valores.\n","    self.values = np.zeros((self.policy.shape[0],))"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"10f2140cd14944a73134fd6b27355332","grade":false,"grade_id":"cell-6d3d7cea2ae55708","locked":true,"schema_version":3,"solution":false,"task":false},"id":"PN0eIzqOPcZp"},"source":["# agent_start()\n","\n","Em agent_start(), escolhemos uma ação com base no estado inicial e na política que estamos avaliando. Também armazenamos o estado em cache para que possamos atualizar posteriormente seu valor quando realizarmos uma atualização de diferença temporal. Por fim, retornamos a ação escolhida para que o loop RL possa continuar e o ambiente possa executar esta ação."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"6f2f7c78f1156b50a122f009d908a1f3","grade":false,"grade_id":"cell-b7179e38fe75348d","locked":true,"schema_version":3,"solution":false,"task":false},"id":"lK5gxkujPcZp"},"outputs":[],"source":["%%add_to TDAgent\n","\n","def agent_start(self, state):\n","    \"\"\"\n","    O primeiro método chamado quando o episódio começa,\n","    ele é chamado  e depois o ambiente começa.\n","    Argumentos:\n","    state (matriz Numpy): o estado da função env_start do ambiente.\n","    Retorna:\n","    A primeira ação que o agente realiza.\n","    \"\"\"\n","    # A política pode ser representada como uma matriz (# Estados, # Ações). Então, podemos usar\n","    # a segunda dimensão aqui ao escolher uma ação.\n","    action = self.rand_generator.choice(range(self.policy.shape[1]), p=self.policy[state])\n","    self.last_state = state\n","    return action"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"0a30618c839091f6b549cbdecdf64542","grade":false,"grade_id":"cell-70200563dea310fa","locked":true,"schema_version":3,"solution":false,"task":false},"id":"rJ8dcr5qPcZp"},"source":["## *Implement* agent_step()\n","\n","Em agent_step(), o agente deve:\n","\n","- Realizar uma atualização para melhorar a estimativa de valor do estado visitado anteriormente, e\n","- Atuar com base no estado proporcionado pelo meio ambiente.\n","\n","A última das duas etapas acima foi implementada para você. Implemente o primeiro. Observe que, diferentemente de Agent_end(), o episódio ainda não terminou em Agent_step(). em outras palavras, o estado observado anteriormente não era um estado terminal."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"bc6bb81efeca49f2fbbe857b53c911c1","grade":false,"grade_id":"cell-54079abb73dd10b4","locked":false,"schema_version":3,"solution":true,"task":false},"id":"c8_3zgg2PcZq"},"outputs":[],"source":["%%add_to TDAgent\n","\n","def agent_step(self, reward, state):\n","    \"\"\"\n","    Um passo dado pelo agente.\n","    Argumentos:\n","    recompensa (float): a recompensa recebida pela última ação realizada\n","    estado (matriz Numpy): o estado do\n","    etapa do ambiente após a última ação, ou seja, onde o agente foi parar após a\n","    última ação\n","    Retorna:\n","    A ação que o agente está realizando.\n","    \"\"\"\n","\n","    # Dica: Devemos realizar uma atualização com o último estado visto que agora temos a recompensa e\n","    # próximo estado. Dividimos isso em duas etapas. Lembre-se, por exemplo, que a atualização de Monte-Carlo\n","    # tinha a forma: V[S_t] = V[S_t] + alpha * (target - V[S_t]), onde o alvo era o retorno, G_t.\n","\n","    # Formula de atualização TD(0): V[S_t] = V[S_t] + alpha * (reward + gamma * V[S_t+1] - V[S_t])\n","\n","    last_state = self.last_state\n","    #  V[S_t] += alpha * (reward + discount * V[S_t+1] - V[S_t])\n","    #  V[S_t] += self.step_size * (reward + self.discount * self.values[state] - self.values[last_state])\n","    # self.values[last_state] += self.step_size * (reward + self.discount * self.values[state] - self.values[last_state])\n","    td_error = reward + self.discount * self.values[state] - self.values[last_state]\n","    self.values[last_state] += self.step_size * td_error\n","\n","    # Tendo atualizado o valor do último estado, agora agimos com base no atual\n","    # estado e defina o último estado como o atual, pois a seguir faremos um\n","    # atualize com ele quando agent_step for chamado em seguida, uma vez que a ação que retornamos desta função\n","    # é executado no ambiente.\n","\n","    action = self.rand_generator.choice(range(self.policy.shape[1]), p=self.policy[state])\n","    self.last_state = state\n","\n","    return action"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"489573a07301536614c0c3a68d74f0b1","grade":false,"grade_id":"cell-b66d9ed60950f928","locked":true,"schema_version":3,"solution":false,"task":false},"id":"5f07R9twPcZq"},"source":["## *Implement* agent_end()\n","\n","Implement the TD update for the case where an action leads to a terminal state."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"21b0563eedd718ae37cc451cfe50423a","grade":false,"grade_id":"cell-f47463c94223a7fa","locked":false,"schema_version":3,"solution":true,"task":false},"id":"PyvQ0OnxPcZq"},"outputs":[],"source":["%%add_to TDAgent\n","\n","def agent_end(self, reward):\n","    \"\"\"\n","    Executar quando o agente terminar.\n","    Argumentos:\n","    recompensa (float): a recompensa que o agente recebeu por entrar no estado terminal.\n","    \"\"\"\n","\n","    # Dica: Aqui também devemos realizar uma atualização com o último estado visto que agora temos a recompensa\n","    last_state = self.last_state\n","    # Observe que, neste caso, a ação levou à termination.\n","    # Mais uma vez, dividimos isso em duas etapas\n","    # Calculando o alvo e a própria atualização que usa o alvo\n","    # e a estimativa do valor atual para o estado cujo valor estamos atualizando.\n","    td_error = reward - self.values[last_state]\n","    self.values[last_state] += self.step_size * td_error"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"0c53e1942168f1f625116dcf4445852a","grade":false,"grade_id":"cell-29b13c18d6b94737","locked":true,"schema_version":3,"solution":false,"task":false},"id":"DDyUFR1oPcZr"},"source":["## agent_cleanup()\n","\n","In cleanup, we simply reset the last state to be None to ensure that we are not storing any states past an episode."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"5ddea78d8e68fcbc0210e88d35eb84dd","grade":false,"grade_id":"cell-35c6057f39236706","locked":true,"schema_version":3,"solution":false,"task":false},"id":"2VQFzVKOPcZr"},"outputs":[],"source":["%%add_to TDAgent\n","\n","def agent_cleanup(self):\n","    \"\"\"Cleanup done after the agent ends.\"\"\"\n","    self.last_state = None"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"0b976f8ba3941c94436c3e56b440a600","grade":false,"grade_id":"cell-ea675d282349503b","locked":true,"schema_version":3,"solution":false,"task":false},"id":"MUvXFb0BPcZr"},"source":["## agent_message()\n","\n","agent_message() can generally be used to get different kinds of information about an RLGlue agent in the interaction loop of RLGlue. Here, we conditonally check for a message matching \"get_values\" and use it to retrieve the values table the agent has been updating over time."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"661f3a352859f22e5581f2953a6738ec","grade":false,"grade_id":"cell-8bac91a36a2cab6f","locked":true,"schema_version":3,"solution":false,"task":false},"id":"bKnQ9dC0PcZr"},"outputs":[],"source":["%%add_to TDAgent\n","\n","def agent_message(self, message):\n","    \"\"\"A function used to pass information from the agent to the experiment.\n","    Args:\n","        message: The message passed to the agent.\n","    Returns:\n","        The response (or answer) to the message.\n","    \"\"\"\n","    if message == \"get_values\":\n","        return self.values\n","    else:\n","        raise Exception(\"TDAgent.agent_message(): Message not understood!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uPx9BKLkPcZs"},"outputs":[],"source":["# Feel free to make any changes to this cell to debug your code\n","\n","# The following test checks that the TD check works for a case where the transition\n","# garners reward -1 and does not lead to a terminal state. This is in a simple two state setting\n","# where there is only one action. The first state's current value estimate is 0 while the second is 1.\n","# Note the discount and step size if you are debugging this test.\n","agent = TDAgent()\n","policy_list = np.array([[1.], [1.]])\n","agent.agent_init({\"policy\": np.array(policy_list), \"discount\": 0.99, \"step_size\": 0.1})\n","agent.values = np.array([0., 1.])\n","agent.agent_start(0)\n","\n","reward = -1\n","next_state = 1\n","agent.agent_step(reward, next_state)\n","\n","assert(np.isclose(agent.values[0], -0.001) and np.isclose(agent.values[1], 1.))\n","\n","# The following test checks that the TD check works for a case where the transition\n","# garners reward -100 and lead to a terminal state. This is in a simple one state setting\n","# where there is only one action. The state's current value estimate is 0.\n","# Note the discount and step size if you are debugging this test.\n","agent = TDAgent()\n","policy_list = np.array([[1.]])\n","agent.agent_init({\"policy\": np.array(policy_list), \"discount\": 0.99, \"step_size\": 0.1})\n","agent.values = np.array([0.])\n","agent.agent_start(0)\n","\n","reward = -100\n","next_state = 0\n","agent.agent_end(reward)\n","\n","assert(np.isclose(agent.values[0], -10))"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"6afbe1c030cc5d88088b8baf8055d3a5","grade":true,"grade_id":"cell-63cd42d5248e7e84","locked":true,"points":20,"schema_version":3,"solution":false,"task":false},"id":"fbplQCChPcZs"},"outputs":[],"source":["agent = TDAgent()\n","policy_list = [np.random.dirichlet(np.ones(10), size=1).squeeze() for _ in range(100)]\n","\n","for n in range(100):\n","    gamma = np.random.random()\n","    alpha = np.random.random()\n","    agent.agent_init({\"policy\": np.array(policy_list), \"discount\": gamma, \"step_size\": alpha})\n","    agent.values = np.random.randn(*agent.values.shape)\n","    state = np.random.randint(100)\n","    agent.agent_start(state)\n","\n","    for _ in range(100):\n","        prev_agent_vals = agent.values.copy()\n","        reward = np.random.random()\n","        if np.random.random() > 0.1:\n","            next_state = np.random.randint(100)\n","            agent.agent_step(reward, next_state)\n","            prev_agent_vals[state] = prev_agent_vals[state] + alpha * (reward + gamma * prev_agent_vals[next_state] - prev_agent_vals[state])\n","            assert(np.allclose(prev_agent_vals, agent.values))\n","            state = next_state\n","        else:\n","            agent.agent_end(reward)\n","            prev_agent_vals[state] = prev_agent_vals[state] + alpha * (reward - prev_agent_vals[state])\n","            assert(np.allclose(prev_agent_vals, agent.values))\n","            break"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"e4813fc1b7e51343c87bd7ae96d6a199","grade":false,"grade_id":"cell-957429a27572ea22","locked":true,"schema_version":3,"solution":false,"task":false},"id":"ifrv6ZT5PcZs"},"source":["## Section 3. Policy Evaluation Experiments\n","\n","Finally, in this last part of the assignment, you will get to see the TD policy evaluation algorithm in action by looking at the estimated values, the per state value error and after the experiment is complete, the Mean Squared Value Error curve vs. episode number, summarizing how the value error changed over time.\n","\n","The code below runs one run of an experiment given env_info and agent_info dictionaries. A \"manager\" object is created for visualizations and is used in part for the autograder. By default, the run will be for 5000 episodes. The true_values_file is specified to compare the learned value function with the values stored in the true_values_file. Plotting of the learned value  function occurs by default after every 100 episodes. In addition, when true_values_file is specified, the value error per state and the root mean square value error will also be plotted."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"7dd158868b6da83ed2d8af5393fa19e7","grade":false,"grade_id":"cell-aa30396771314085","locked":true,"schema_version":3,"solution":false,"task":false},"id":"hIJUFercPcZs"},"outputs":[],"source":["%matplotlib notebook\n","\n","def run_experiment(env_info, agent_info,num_episodes=5000, experiment_name=None, plot_freq=100, true_values_file=None, value_error_threshold=1e-8):\n","    env = CliffWalkEnvironment\n","    agent = TDAgent\n","    rl_glue = RLGlue(env, agent)\n","\n","    rl_glue.rl_init(agent_info, env_info)\n","\n","    manager = Manager(env_info, agent_info, true_values_file=true_values_file, experiment_name=experiment_name)\n","    for episode in range(1, num_episodes + 1):\n","        rl_glue.rl_episode(0) # no step limit\n","        if episode % plot_freq == 0:\n","            values = rl_glue.agent.agent_message(\"get_values\")\n","            manager.visualize(values, episode)\n","\n","    values = rl_glue.agent.agent_message(\"get_values\")\n","    return values"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"e726e3aa91859d231913a83be7b376f6","grade":false,"grade_id":"cell-f8a8ad5275cc7e7f","locked":true,"schema_version":3,"solution":false,"task":false},"id":"fRHbWD1hPcZ1"},"source":["The cell below just runs a policy evaluation experiment with the determinstic optimal policy that strides just above the cliff. You should observe that the per state value error and RMSVE curve asymptotically go towards 0. The arrows in the four directions denote the probabilities of taking each action. This experiment is ungraded but should serve as a good test for the later experiments. The true values file provided for this experiment may help with debugging as well."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"c9935c904c8dac62540410b7dc0eb740","grade":false,"grade_id":"cell-2a4f093ff50135b8","locked":true,"schema_version":3,"solution":false,"task":false},"id":"0Eof3r7yPcZ2"},"outputs":[],"source":["env_info = {\"grid_height\": 4, \"grid_width\": 12, \"seed\": 0}\n","\n","agent_info = {\"discount\": 1, \"step_size\": 0.01, \"seed\": 0}\n","\n","# The Optimal Policy that strides just along the cliff\n","policy = np.ones(shape=(env_info['grid_width'] * env_info['grid_height'], 4)) * 0.25\n","\n","policy[36] = [1, 0, 0, 0]\n","for i in range(24, 35):\n","    policy[i] = [0, 0, 0, 1]\n","policy[35] = [0, 0, 1, 0]\n","\n","agent_info.update({\"policy\": policy})"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"d288407bc4dd094efc56faebad142e5f","grade":false,"grade_id":"cell-41ea2c684571e6e8","locked":false,"schema_version":3,"solution":true,"task":false},"id":"AP7n22EKPcZ2"},"outputs":[],"source":["# A Política Segura\n","# Dica: Preencha a matriz abaixo (conforme feito na célula anterior) com base na ilustração da política segura\n","# no diagrama de ambiente. Esta é a política que se afasta o mais possível do precipício.\n","# Chamamos-lhe uma política \"segura\" porque se o ambiente tivesse alguma estocasticidade, esta política faria um bom trabalho em\n","# evitando que o agente caia no precipício (em contraste com a política ótima mostrada anteriormente).\n","\n","# build a uniform random policy\n","policy = np.ones(shape=(env_info['grid_width'] * env_info['grid_height'], 4)) * 0.25\n","\n","# build an example environment\n","env = CliffWalkEnvironment()\n","env.env_init(env_info)\n","\n","# modify the uniform random policy\n","# your code here\n","# build a uniform random policy\n","policy = np.ones(shape=(env_info['grid_width'] * env_info['grid_height'], 4)) * 0.25\n","\n","# [0: Cima, 1: Esquerda, 2: Baixo, 3: Direita]\n","\n","# modify the uniform random policy for safe policy\n","for state in range(env_info['grid_width'] * env_info['grid_height']):\n","    row, col = state // env_info['grid_width'], state % env_info['grid_width']\n","    if row > 0:                            # Se não estiver na primeira linha\n","        policy[state][0] += 0.1            # Adiciona 0.1 à probabilidade de subir\n","    if row < env_info['grid_height'] - 1:  # Se não estiver na última linha\n","        policy[state][1] += 0.1            # Adiciona 0.1 à probabilidade de descer\n","    if col > 0:                            # Se não estiver na primeira coluna\n","        policy[state][2] += 0.1            # Adiciona 0.1 à probabilidade de mover para a esquerda\n","    if col < env_info['grid_width'] - 1:   # Se não estiver na última coluna\n","        policy[state][3] += 0.1            # Adiciona 0.1 à probabilidade de mover para a direita\n","\n","# Normaliza as probabilidades para que a soma seja igual a 1\n","policy /= np.sum(policy, axis=1, keepdims=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"7d6c1c46ebd8a20c291b6c485601a7da","grade":true,"grade_id":"cell-747a2fdf30624260","locked":true,"points":10,"schema_version":3,"solution":false,"task":false},"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"f-QQNe0BPcZ2","executionInfo":{"status":"error","timestamp":1718317423485,"user_tz":180,"elapsed":12,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}},"outputId":"04806acb-847f-4fce-fd2e-59c121e27815"},"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-9c1ec4651828>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# go up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# top of space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: "]}],"source":["# The contents of the cell will be tested by the autograder.\n","# If they do not pass here, they will not pass there.\n","\n","width = env_info['grid_width']\n","height = env_info['grid_height']\n","\n","# left side of space\n","for x in range(1, height):\n","    s = env.state((x, 0))\n","\n","    # go up\n","    assert np.all(policy[s] == [1, 0, 0, 0])\n","\n","# top of space\n","for y in range(width - 1):\n","    s = env.state((0, y))\n","\n","    # go right\n","    assert np.all(policy[s] == [0, 0, 0, 1])\n","\n","# right side of space\n","for x in range(height - 1):\n","    s = env.state((x, width - 1))\n","\n","    # go down\n","    assert np.all(policy[s] == [0, 0, 1, 0])"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"f61945e3fa9693a71c67308e390831f2","grade":false,"grade_id":"cell-4cff4aa8310f733e","locked":true,"schema_version":3,"solution":false,"task":false},"id":"8_xLjJWTPcZ2"},"outputs":[],"source":["agent_info.update({\"policy\": policy})\n","v = run_experiment(env_info, agent_info, experiment_name=\"Policy Evaluation On Safe Policy\", num_episodes=5000, plot_freq=500)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"fa33604922343541caab97a67b3d0b97","grade":false,"grade_id":"cell-e1cf8e11c672a07e","locked":true,"schema_version":3,"solution":false,"task":false},"id":"2xb5XmMSPcZ3"},"outputs":[],"source":["# A Near Optimal Stochastic Policy\n","# Now, we try a stochastic policy that deviates a little from the optimal policy seen above.\n","# This means we can get different results due to randomness.\n","# We will thus average the value function estimates we get over multiple runs.\n","# This can take some time, upto about 5 minutes from previous testing.\n","# NOTE: The autograder will compare . Re-run this cell upon making any changes.\n","\n","env_info = {\"grid_height\": 4, \"grid_width\": 12}\n","agent_info = {\"discount\": 1, \"step_size\": 0.01}\n","\n","policy = np.ones(shape=(env_info['grid_width'] * env_info['grid_height'], 4)) * 0.25\n","policy[36] = [0.9, 0.1/3., 0.1/3., 0.1/3.]\n","for i in range(24, 35):\n","    policy[i] = [0.1/3., 0.1/3., 0.1/3., 0.9]\n","policy[35] = [0.1/3., 0.1/3., 0.9, 0.1/3.]\n","agent_info.update({\"policy\": policy})\n","agent_info.update({\"step_size\": 0.01})"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"b3888898a45dcda0c5420f24132be6d4","grade":false,"grade_id":"cell-6038bf336a6d7fd2","locked":true,"schema_version":3,"solution":false,"task":false},"id":"qliWJVOSPcZ3"},"outputs":[],"source":["env_info['seed'] = 0\n","agent_info['seed'] = 0\n","v = run_experiment(env_info, agent_info,\n","               experiment_name=\"Policy Evaluation On Optimal Policy\",\n","               num_episodes=5000, plot_freq=100)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"fd6c272cda343e1a77596b771469ecc6","grade":false,"grade_id":"cell-15c20060eb691e3d","locked":true,"schema_version":3,"solution":false,"task":false},"id":"NTamuoQOPcZ3"},"source":["## Wrapping Up\n","Congratulations, you have completed assignment 2! In this assignment, we investigated a very useful concept for sample-based online learning: temporal difference. We particularly looked at the prediction problem where the goal is to find the value function corresponding to a given policy. In the next assignment, by learning the action-value function instead of the state-value function, you will get to see how temporal difference learning can be used in control as well."]}],"metadata":{"coursera":{"course_slug":"sample-based-learning-methods","graded_item_id":"P4k5f","launcher_item_id":"OwIbv"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}