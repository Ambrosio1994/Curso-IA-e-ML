{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOiVE0IQrGR6d5R5d+cSrHQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRqCvedbMNuC","executionInfo":{"status":"ok","timestamp":1722600018067,"user_tz":180,"elapsed":110500,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}},"outputId":"15424b22-121f-4c28-d6a2-b11bfaa321a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-08-02 11:58:26--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\n","Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n","Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2598656062 (2.4G) [application/zip]\n","Saving to: ‘Positive_tensors.zip’\n","\n","Positive_tensors.zi 100%[===================>]   2.42G  22.6MB/s    in 1m 50s  \n","\n","2024-08-02 12:00:17 (22.6 MB/s) - ‘Positive_tensors.zip’ saved [2598656062/2598656062]\n","\n"]}]},{"cell_type":"code","source":["!unzip -q Positive_tensors.zip"],"metadata":{"id":"85du_P92MNwo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6hFSGUtUMNzd","executionInfo":{"status":"ok","timestamp":1722600222330,"user_tz":180,"elapsed":59272,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}},"outputId":"99c4aa2a-67d0-4c2d-8a5b-057c301199ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-08-02 12:02:41--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n","Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n","Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2111408108 (2.0G) [application/zip]\n","Saving to: ‘Negative_tensors.zip’\n","\n","Negative_tensors.zi 100%[===================>]   1.97G  30.0MB/s    in 58s     \n","\n","2024-08-02 12:03:40 (34.6 MB/s) - ‘Negative_tensors.zip’ saved [2111408108/2111408108]\n","\n"]}]},{"cell_type":"code","source":["!unzip -q Negative_tensors.zip"],"metadata":{"id":"tXvVIcIGNU5D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install torchvision"],"metadata":{"id":"tkKSnU9TMN1_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torchvision.models as models\n","from PIL import Image\n","import pandas\n","from torchvision import transforms\n","import torch.nn as nn\n","import time\n","import torch\n","import matplotlib.pylab as plt\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","import h5py\n","import os\n","import glob\n","from matplotlib.pyplot import imshow\n","torch.manual_seed(0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yn8YIYzVMN5r","executionInfo":{"status":"ok","timestamp":1722600391333,"user_tz":180,"elapsed":10568,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}},"outputId":"2cf6226b-553f-49d2-e7c9-772754c62367"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7cec9412fa70>"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# Create your own dataset object\n","\n","class Dataset(Dataset):\n","\n","    # Constructor\n","    def __init__(self,transform=None,train=True):\n","        directory=\"/content/\"\n","        positive=\"Positive_tensors\"\n","        negative='Negative_tensors'\n","\n","        positive_file_path=os.path.join(directory,positive)\n","        negative_file_path=os.path.join(directory,negative)\n","        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n","        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n","        number_of_samples=len(positive_files)+len(negative_files)\n","        self.all_files=[None]*number_of_samples\n","        self.all_files[::2]=positive_files\n","        self.all_files[1::2]=negative_files\n","        # The transform is goint to be used on image\n","        self.transform = transform\n","        #torch.LongTensor\n","        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n","        self.Y[::2]=1\n","        self.Y[1::2]=0\n","\n","        if train:\n","            self.all_files=self.all_files[0:30000]\n","            self.Y=self.Y[0:30000]\n","            self.len=len(self.all_files)\n","        else:\n","            self.all_files=self.all_files[30000:]\n","            self.Y=self.Y[30000:]\n","            self.len=len(self.all_files)\n","\n","    # Get the length\n","    def __len__(self):\n","        return self.len\n","\n","    # Getter\n","    def __getitem__(self, idx):\n","\n","        image=torch.load(self.all_files[idx])\n","        y=self.Y[idx]\n","\n","        # If there is any transform method, apply it onto the image\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, y"],"metadata":{"id":"vOhzRQbSMN_I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = Dataset(train=True)\n","validation_dataset = Dataset(train=False)\n","print(\"done\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1VEV8HunMOCD","executionInfo":{"status":"ok","timestamp":1722600391334,"user_tz":180,"elapsed":12,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}},"outputId":"f96b40a4-5e6b-44f7-cb79-e306f7ee28e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["done\n"]}]},{"cell_type":"markdown","source":["<b>Prepare a pre-trained resnet18 model :</b>"],"metadata":{"id":"yoIJ8kAFMOEx"}},{"cell_type":"markdown","source":["<b>Etapa 1</b>: Carregar o modelo pré-treinado <code>resnet18</code> Defina o parâmetro <code>pretrained</code> como verdadeiro:"],"metadata":{"id":"OlmKJt_RMOHJ"}},{"cell_type":"code","source":["# Step 1: Carregue o modelo pré-treinado resnet18\n","model = models.resnet18(pretrained=True)\n","# Type your code here"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OouStu0DMOJu","executionInfo":{"status":"ok","timestamp":1722600393200,"user_tz":180,"elapsed":1874,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}},"outputId":"19bcb807-8237-4280-ca6b-da31f6cbe1e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 63.1MB/s]\n"]}]},{"cell_type":"code","source":["# Etapa 2: Definir o parâmetro que não pode ser treinado para o modelo pré-treinado\n","for param in model.parameters():\n","  param.requires_grad = False\n","# Type your code here"],"metadata":{"id":"nFMmeDtdMOPG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<code>resnet18</code> é usado para classificar 1000 objetos diferentes; como resultado, a última camada tem 1.000 saídas. As 512 entradas vêm do fato de que a camada anteriormente oculta possui 512 saídas."],"metadata":{"id":"z9wDyyIhQPbb"}},{"cell_type":"markdown","source":["<b>Etapa 3</b>: Substitua a camada de saída <code>model.fc</code> da rede neural por um objeto <code>nn.Linear</code>, para classificar 2 classes diferentes. Para os parâmetros <code>in_features </code> lembre-se que a última camada oculta possui 512 neurônios."],"metadata":{"id":"v4v0y7wJQUZG"}},{"cell_type":"markdown","source":["Imprima o modelo para mostrar se você acertou a resposta.<br> <b>(Seu revisor avaliará com base no que você imprimir aqui.)</b>"],"metadata":{"id":"8WLL-V-BQ_1V"}},{"cell_type":"code","source":["model.fc = nn.Linear(512, 2)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nfIp4ONoMORo","executionInfo":{"status":"ok","timestamp":1722600393203,"user_tz":180,"elapsed":26,"user":{"displayName":"DIEGO AMBROSIO","userId":"04645390355005059837"}},"outputId":"ffa7f6bf-ea02-43a7-d49b-91e1b84fa255"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=2, bias=True)\n",")\n"]}]},{"cell_type":"markdown","source":["Nesta questão você treinará seu modelo:"],"metadata":{"id":"Fv62UdNTRXwA"}},{"cell_type":"markdown","source":["<b>Etapa 1</b>: Crie uma função de critério de entropia cruzada"],"metadata":{"id":"1dxvLrMRRXzU"}},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam([param for param in model.parameters() if param.requires_grad], lr=0.003)"],"metadata":{"id":"G97AoW11RXOD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<b>Etapa 2</b>: Crie um carregador de treinamento e um objeto carregador de validação, o tamanho do lote deve ter 100 amostras cada."],"metadata":{"id":"nJ0HfPlTSaE8"}},{"cell_type":"code","source":["batch_size = 100\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size)\n","validation_loader = DataLoader(dataset=validation_dataset, batch_size=batch_size)"],"metadata":{"id":"FZoud2iEMOUA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_epochs=1\n","loss_list=[]\n","accuracy_list=[]\n","N_test=len(validation_dataset)\n","N_train=len(train_dataset)\n","start_time = time.time()\n","\n","for epoch in range(n_epochs):\n","    for x, y in train_loader:\n","\n","        model.train()\n","        #clear gradient\n","        optimizer.zero_grad()\n","        #make a prediction\n","        z = model(x)\n","        # calculate loss\n","        loss = criterion(z, y)\n","        # calculate gradients of parameters\n","        loss.backward()\n","        # update parameters\n","        optimizer.step()\n","        loss_list.append(loss.data)\n","    correct=0\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for x_test, y_test in validation_loader:\n","            z = model(x_test)  # Faz a predição\n","            _, yhat = torch.max(z.data, 1)  # Obtém a predição\n","            correct += (yhat == y_test).sum().item()  # Conta as predições corretas\n","\n","    accuracy = correct / N_test\n","    accuracy_list.append(accuracy)"],"metadata":{"id":"IprGvoK2MOWU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracy"],"metadata":{"id":"xq4qBQu1MOY5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(loss_list)\n","plt.plot(accuracy_list)\n","plt.xlabel(\"iteration\")\n","plt.ylabel(\"loss\")\n","plt.show()"],"metadata":{"id":"Xy32EigMXrrq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(accuracy_list)\n","plt.xlabel(\"iteration\")\n","plt.ylabel(\"accuracy\")\n","plt.show()"],"metadata":{"id":"tFi233Cd3RjE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<b>Identifique as primeiras quatro amostras classificadas incorretamente usando os dados de validação:</b>"],"metadata":{"id":"5fJpDbqmXrny"}},{"cell_type":"code","source":["model.eval()\n","incorrect_samples = []\n","correct = 0\n","\n","with torch.no_grad():\n","    for x_test, y_test in validation_loader:\n","        z = model(x_test)  # Make prediction\n","        _, yhat = torch.max(z.data, 1)  # Get prediction\n","        correct += (yhat == y_test).sum().item()  # Count correct predictions\n","\n","        # Store incorrect samples\n","        incorrect_indices = (yhat != y_test).nonzero(as_tuple=True)[0]\n","        for idx in incorrect_indices:\n","            if len(incorrect_samples) < 4:  # Stores only the first 4\n","                incorrect_samples.append((x_test[idx], y_test[idx], yhat[idx]))"],"metadata":{"id":"DcBezENK0nwD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np  # Import NumPy for transpose\n","\n","# ... your code for incorrect_samples\n","\n","fig, axs = plt.subplots(1, 4, figsize=(12, 3))\n","for i, (img, true_label, predicted_label) in enumerate(incorrect_samples):\n","  img = img.squeeze()  # Remove extra dimension if exists\n","\n","  # Check if grayscale or color image\n","  if len(img.shape) == 2:\n","    # Grayscale image, use 'gray' cmap\n","    axs[i].imshow(img.cpu().numpy(), cmap='gray')\n","  else:\n","    # Color image, rearrange channels (assuming RGB or BGR) and use default cmap\n","    img = img.cpu().numpy().transpose((1, 2, 0))  # Move channel dimension to last\n","    axs[i].imshow(img)\n","\n","  axs[i].set_title(f\"True: {true_label.item()}\\nPred: {predicted_label.item()}\")\n","  axs[i].axis('off')\n","\n","plt.show()"],"metadata":{"id":"oswk9jpUXrlC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(4):\n","  print(incorrect_samples[i][1], incorrect_samples[i][2])"],"metadata":{"id":"uly5kMXC3mbk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["incorrect_samples.index"],"metadata":{"id":"sbE6MS3s3mxN"},"execution_count":null,"outputs":[]}]}